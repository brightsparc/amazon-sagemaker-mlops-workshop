{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Ludwig base Docker Image\n",
    "\n",
    "We will start our MLOps journey here by creating an abstract Docker Image for supporting [Ludwig](https://uber.github.io/ludwig/) models.\n",
    "\n",
    "So, after we create and test locally our Dockerfile, we'll send it to our first pipeline that will build this image and make it available in ECR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First create a basic application that will host our model code\n",
    "\n",
    "Create a WebService application with two methods: **ping** and **invocations**. Ping is for healthcheck and invocations is for calling your model.\n",
    "\n",
    "We'll use a Sagemaker feature called \"CustomAttributes\" for indicating if we want to include Headers in response.\n",
    "\n",
    "For a production environment it is important to use a **WSGI** solution. We will use a combo of **bottle** and **bjoern**. Bottle is our webservice api and bjoern our WSGI server. Since bjoern is single threaded, you can't run multiple predictions at the same time. If you need something like that, maybe you need gunicorn and a reverse proxy to protect your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Import python serving\n",
    "import bjoern\n",
    "import bottle\n",
    "from bottle import run, request, post, get, HTTPResponse\n",
    "\n",
    "# Import ludwig library\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import ludwig\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "# Import sagemaker experiments API\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "# Supress TF Future Warnings\n",
    "import warnings  \n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    \n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('ludwig version: {}'.format(ludwig.__version__))\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # parameters for training (TODO: Add horovod etc)\n",
    "    parser.add_argument('--experiment_name', type=str, default='sagemaker_experiment')\n",
    "    parser.add_argument('--trial_name', type=str, default='run')\n",
    "    parser.add_argument('--pandas_engine', type=str, default='python')\n",
    "    parser.add_argument('--pandas_header', type=bool, default=False)\n",
    "    parser.add_argument('--pandas_float_format', type=str, default='%.10f')\n",
    "    \n",
    "    # data directories\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN', '/opt/ml/input/data/training'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION', '/opt/ml/input/data/validation'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST', '/opt/ml/input/data/test'))\n",
    "\n",
    "    # input_config\n",
    "    parser.add_argument('--config_dir', type=str, default=os.environ.get('SM_INPUT_CONFIG_DIR', '/opt/ml/input/config'))\n",
    "\n",
    "    # model directory: we will use the default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument('--output_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR', '/opt/ml/output/data'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', '/opt/ml/model'))\n",
    "    \n",
    "    return parser.parse_known_args()   \n",
    "\n",
    "def read_csv_dataframe(path, engine='python'):\n",
    "    files = glob.glob(os.path.join(path, '*.csv'))\n",
    "    if len(files) > 0:\n",
    "        return pd.concat([pd.read_csv(fn, engine=engine) for fn in files], axis=0, ignore_index=True)\n",
    "\n",
    "def ludwig_train():\n",
    "    args, _ = parse_args()\n",
    "    print(args)\n",
    "    \n",
    "    # Create model from definition\n",
    "    ludwig_model = LudwigModel(None, model_definition_file='model_definition.yml')\n",
    "    print('Input Features:\\n{}'.format(json.dumps(ludwig_model.model_definition['input_features'])))\n",
    "    print('Output Features:\\n{}'.format(json.dumps(ludwig_model.model_definition['output_features'])))\n",
    "    \n",
    "    # Allow specifying training hyperparameters \n",
    "    # see: https://uber.github.io/ludwig/user_guide/#training\n",
    "    trainnig_config_path = os.path.join(args.config_dir, 'hyperparameters.config')\n",
    "    if os.path.exists(trainnig_config_path):\n",
    "        with open(trainnig_config_path, 'r') as tc:\n",
    "            hyperparameters = json.load(tc)\n",
    "            ludwig_model.model_definition['training'] = \\\n",
    "                dict(ludwig_model.model_definition['training'], **hyperparameters)            \n",
    "    print('Training params:\\n{}'.format(json.dumps(ludwig_model.model_definition['training'])))\n",
    "\n",
    "    # Load the train/validation/test files\n",
    "    data_train_df = read_csv_dataframe(args.train, engine=args.pandas_engine)\n",
    "    data_validation_df = read_csv_dataframe(args.validation, engine=args.pandas_engine)\n",
    "    data_test_df = read_csv_dataframe(args.test, engine=args.pandas_engine)\n",
    "    \n",
    "    print('Training model...')\n",
    "    train_stats = ludwig_model.train(\n",
    "        skip_save_log=True, # Don't save tensorboard\n",
    "        skip_save_processed_input=True, # Don't save pre-processed input\n",
    "        data_train_df=data_train_df,\n",
    "        data_validation_df=data_validation_df,\n",
    "        data_test_df=data_test_df,\n",
    "        output_directory=args.output_dir,\n",
    "        experiment_name=args.experiment_name,\n",
    "        model_name=args.trial_name\n",
    "    )\n",
    "    \n",
    "    # Save the ludwig model \n",
    "    ludwig_model.save(args.model_dir)\n",
    "    \n",
    "#     # Optionally save the model for serving in a numbered directory\n",
    "#     saved_model_path = os.path.join(args.model_dir, str(int(time.time())))\n",
    "#     ludwig_model.save_for_serving(saved_model_path)\n",
    "    \n",
    "    with open(os.path.join(args.output_dir, 'train_stats.json'), 'w') as tsf:    \n",
    "        json.dump(train_stats, tsf, cls=NpEncoder)        \n",
    "\n",
    "    if train_stats['train']['combined']['loss']:\n",
    "        print('Train Loss: {};'.format(train_stats['train']['combined']['loss'][-1]))\n",
    "        print('Train Accuracy: {:.5%};'.format(train_stats['train']['combined']['accuracy'][-1]))\n",
    "    if train_stats['validation']['combined']['loss']:\n",
    "        print('Validation Loss: {};'.format(train_stats['validation']['combined']['loss'][-1]))\n",
    "        print('Validation Accuracy: {:.5%};'.format(train_stats['validation']['combined']['accuracy'][-1]))\n",
    "    if train_stats['test']['combined']['loss']:\n",
    "        print('Test Loss: {};'.format(train_stats['test']['combined']['loss'][-1]))\n",
    "        print('Test Accuracy: {:.5%};'.format(train_stats['test']['combined']['accuracy']-1))\n",
    "        \n",
    "    ludwig_model.close()    \n",
    "        \n",
    "ludwig_model = None\n",
    "args = None\n",
    "\n",
    "def load_model():\n",
    "    global ludwig_model\n",
    "    global args\n",
    "    if ludwig_model == None:\n",
    "        # Load model and print definition if not already loaded\n",
    "        args, _ = parse_args()\n",
    "        print('Args', args)\n",
    "        print('Loading model...')\n",
    "        ludwig_model = LudwigModel.load(args.model_dir)\n",
    "    return ludwig_model, args\n",
    "    \n",
    "@get('/ping')\n",
    "def ping():\n",
    "    print('ping')\n",
    "    # Load/cache the model on ping\n",
    "    load_model()\n",
    "    # Write back plain text\n",
    "    headers = {'Content-type': 'text/plain'}\n",
    "    raise HTTPResponse(\"OK\",status=200,headers=headers)\n",
    "\n",
    "@post('/invocations')\n",
    "def invoke():\n",
    "    content_type = request.content_type\n",
    "    print('invoke: {}'.format(content_type))\n",
    "    payload = StringIO(request.body.read().decode('utf-8'))\n",
    "    if request.content_type.startswith('text/csv'):\n",
    "        data_df = pd.read_csv(payload)\n",
    "    elif request.content_type.startswith('application/json'):\n",
    "        data_df = pd.read_json(payload)\n",
    "    else:\n",
    "        raise(Exception('Unsupported Content Type: {}'.format(request.content_type)))        \n",
    "    print('shape', data_df.shape)\n",
    "    ludwig_model, _ = load_model()\n",
    "    predictions = ludwig_model.predict(data_df=data_df)\n",
    "    # Write back CSV\n",
    "    headers = {'Content-type': 'text/csv'}\n",
    "    # Use CustomAttributes to determine if we should include the headers\n",
    "    include_header = request.get_header('X-Amzn-SageMaker-Custom-Attributes') == 'Headers' or args.pandas_header\n",
    "    result = predictions.to_csv(index=False, header=include_header, \n",
    "                                float_format=args.pandas_float_format)\n",
    "    raise HTTPResponse(result, status=200, headers=headers)\n",
    "\n",
    "#Class for json nump encoding with int64\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2 or ( not sys.argv[1] in [ \"serve\", \"train\", \"test\"] ):\n",
    "        raise Exception(\"Invalid argument: you must inform 'train' for training mode or 'serve' predicting mode\") \n",
    "\n",
    "    train = sys.argv[1] == \"train\"\n",
    "    test = sys.argv[1] == \"test\"\n",
    "\n",
    "    # TEMP: Print out all files mounted under /opt/ml\n",
    "    print([os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(\"/opt/ml\")) for f in fn])    \n",
    "    \n",
    "    if train:\n",
    "        ludwig_train()\n",
    "    elif test:\n",
    "        # Read and write to local file\n",
    "        print('test', sys.argv[2], sys.argv[3], sys.argv[4])\n",
    "        data_df = read_csv_dataframe(sys.argv[2])\n",
    "        print(data_df.head())\n",
    "        ludwig_model, args = load_model()\n",
    "        predictions, test_stats = ludwig_model.test(data_df=data_df)    \n",
    "        predictions.to_csv(sys.argv[3], index=False, header=args.pandas_header, \n",
    "                           float_format=args.pandas_float_format)\n",
    "        # Write test stats to a file\n",
    "        with open(sys.argv[4], 'w') as tsf:    \n",
    "            json.dump(test_stats, tsf, cls=NpEncoder)        \n",
    "    else:\n",
    "        bjoern.run(bottle.app(), \"0.0.0.0\", 8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, create a Dockerfile\n",
    "\n",
    "Baseon the `tensorflow-training` image from the [local region](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html), and install python libraries for serving inference and `ludwig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM tensorflow:latest\n",
    "    \n",
    "RUN apt-get update -y && apt-get install -y libev-dev\n",
    "RUN pip install bottle bjoern \n",
    "RUN pip install ludwig[text] # 0.2.1\n",
    "RUN python -m spacy download en\n",
    "\n",
    "RUN mkdir -p /opt/program\n",
    "RUN mkdir -p /opt/ml\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "ENV PATH=\"/opt/program:${PATH}\"\n",
    "\n",
    "COPY app.py /opt/program\n",
    "WORKDIR /opt/program\n",
    "\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the image locally, first\n",
    "\n",
    "Test that we can pull the tensorflow training image and then build the local docker.\n",
    "\n",
    "The Dockerfile and buildspec.yaml by default pull the `tensorflow-training` image from `ap-southeast-2` region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-ap-southeast-2}\n",
    "export TF_VERSION=1.15.2-cpu-py3\n",
    "\n",
    "$(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids 763104351884)\n",
    "docker pull 763104351884.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/tensorflow-training:$TF_VERSION\n",
    "docker tag 763104351884.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/tensorflow-training:$TF_VERSION tensorflow:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -f Dockerfile -t $BASE_REPO:${IMAGE_TAG:-latest} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finally, let's create the buildspec\n",
    "\n",
    "This file will be used by CodeBuild for creating our base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile buildspec.yml\n",
    "version: 0.2\n",
    "\n",
    "env:\n",
    "  variables:\n",
    "    TF_VERSION: \"1.14-cpu-py3\"\n",
    "        \n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      docker: 18\n",
    "        \n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo Logging in to Amazon ECR...\n",
    "      - $(aws ecr get-login --no-include-email --region ap-southeast-2 --registry-ids 763104351884) \n",
    "      - $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids $AWS_ACCOUNT_ID)\n",
    "      - echo Pulling tensorflow training base image\n",
    "      - docker pull 763104351884.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/tensorflow-training:$TF_VERSION\n",
    "      - docker tag 763104351884.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/tensorflow-training:$TF_VERSION tensorflow:latest\n",
    "  build:\n",
    "    commands:\n",
    "      - echo Build started on `date` for commit $CODEBUILD_RESOLVED_SOURCE_VERSION\n",
    "      - echo Building the Docker image... \n",
    "      - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .\n",
    "      - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo Build completed on `date`\n",
    "      - echo Pushing the Docker image...\n",
    "      - echo docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - echo $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG > image.url\n",
    "      - cat image.url\n",
    "      - echo Done!\n",
    "artifacts:\n",
    "  files:\n",
    "    - image.url\n",
    "  name: image_url\n",
    "  discard-paths: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we push our code to the repo, let's check the building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "repo_name=os.environ['BASE_REPO']\n",
    "region = session.region_name\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "image_tag='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tests\n",
    "!cp app.py Dockerfile buildspec.yml tests/\n",
    "with open('tests/vars.env', 'w') as f:\n",
    "    f.write(\"AWS_ACCOUNT_ID=%s\\n\" % account_id)\n",
    "    f.write(\"IMAGE_TAG=%s\\n\" % image_tag)\n",
    "    f.write(\"IMAGE_REPO_NAME=%s\\n\" % repo_name)\n",
    "    f.write(\"AWS_DEFAULT_REGION=%s\\n\" % region)\n",
    "    f.write(\"AWS_ACCESS_KEY_ID=%s\\n\" % credentials.access_key)\n",
    "    f.write(\"AWS_SECRET_ACCESS_KEY=%s\\n\" % credentials.secret_key)\n",
    "    f.write(\"AWS_SESSION_TOKEN=%s\\n\" % credentials.token )\n",
    "    f.close()\n",
    "\n",
    "!cat tests/vars.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!/tmp/aws-codebuild/local_builds/codebuild_build.sh \\\n",
    "    -a \"$PWD/tests/output\" \\\n",
    "    -s \"$PWD/tests\" \\\n",
    "    -i \"samirsouza/aws-codebuild-standard:2.0\" \\\n",
    "    -e \"$PWD/tests/vars.env\" \\\n",
    "    -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now it's time to push everything to the correct repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../../../mlops-workshop-images/$BASE_REPO\n",
    "cp $OLDPWD/buildspec.yml $OLDPWD/app.py $OLDPWD/Dockerfile .\n",
    "\n",
    "git add buildspec.yml app.py Dockerfile\n",
    "git commit -a -m \" - files for building a $BASE_REPO image\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, now open the AWS console in another tab and go to the CodePipeline console to see the status of our building pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
