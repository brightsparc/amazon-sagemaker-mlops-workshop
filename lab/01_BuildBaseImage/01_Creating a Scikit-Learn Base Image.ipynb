{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Scikit-Learn base Docker Image\n",
    "\n",
    "We will start our MLOps journey here by creating an abstract Docker Image for supporting [Ludwig](https://uber.github.io/ludwig/) models.\n",
    "\n",
    "So, after we create and test locally our Dockerfile, we'll send it to our first pipeline that will build this image and make it available in ECR.\n",
    "\n",
    "This image will be based on `tensorflow-training`, and install python libraries for serving inference and `ludwig`.\n",
    "\n",
    "## First, lets create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:1.14-cpu-py3\n",
    "    \n",
    "RUN apt-get update -y && apt-get install -y libev-dev\n",
    "RUN pip install bottle bjoern \n",
    "RUN pip install ludwig[text] # 0.2.1\n",
    "RUN python -m spacy download en\n",
    "\n",
    "RUN mkdir -p /opt/program\n",
    "RUN mkdir -p /opt/ml\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "ENV PATH=\"/opt/program:${PATH}\"\n",
    "\n",
    "COPY app.py /opt/program\n",
    "WORKDIR /opt/program\n",
    "\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, the a basic application that will host our model code\n",
    "\n",
    "Please, notice that we're creating a WebService application with two methods: **ping** and **invocations**. Ping is for healthcheck and invocations is for calling your model.\n",
    "\n",
    "For a production environment it is important to use a **WSGI** solution. We will use a combo of **bottle** and **bjoern**. Bottle is our webservice api and bjoern our WSGI server. Since bjoern is single threaded, you can't run multiple predictions at the same time. If you need something like that, maybe you need gunicorn and a reverse proxy to protect your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#Class for json nump encoding with int64\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "# Import python serving\n",
    "import bjoern\n",
    "import bottle\n",
    "from bottle import run, request, post, get\n",
    "\n",
    "# Import ludwig library\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import glob\n",
    "import ludwig\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "print('ludwig: {}'.format(ludwig.__version__))\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # parameters for training (TODO: Add horovod etc)\n",
    "    parser.add_argument('--experiment_name', type=str, default='sagemaker_experiment')\n",
    "    parser.add_argument('--trial_name', type=str, default='run')\n",
    "    parser.add_argument('--pandas_engine', type=str, default='python')\n",
    "    \n",
    "    # data directories\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN', '/opt/ml/input/data/training'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION', '/opt/ml/input/data/validation'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST', '/opt/ml/input/data/test'))\n",
    "\n",
    "    # input_config\n",
    "    parser.add_argument('--config_dir', type=str, default=os.environ.get('SM_INPUT_CONFIG_DIR', '/opt/ml/input/config'))\n",
    "\n",
    "    # model directory: we will use the default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument('--output_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR', '/opt/ml/output/data'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', '/opt/ml/model'))\n",
    "    \n",
    "    return parser.parse_known_args()   \n",
    "\n",
    "def read_csv_dataframe(path, engine='python'):\n",
    "    files = glob.glob(os.path.join(path, '*.csv'))\n",
    "    if len(files) > 0:\n",
    "        return pd.concat([pd.read_csv(fn, engine=engine) for fn in files], axis=0, ignore_index=True)\n",
    "\n",
    "def ludwig_train():\n",
    "    args, _ = parse_args()\n",
    "    print(args)\n",
    "    \n",
    "    # Create model from definition\n",
    "    ludwig_model = LudwigModel(None, model_definition_file='model_definition.yml')\n",
    "    \n",
    "    # Allow specifying training hyperparameters \n",
    "    # see: https://uber.github.io/ludwig/user_guide/#training\n",
    "    trainnig_config_path = os.path.join(args.config_dir, 'hyperparameters.config')\n",
    "    if os.path.exists(trainnig_config_path):\n",
    "        with open(trainnig_config_path, 'r') as tc:\n",
    "            ludwig_model.model_definition['training'] = json.load(tc)\n",
    "\n",
    "    print('model definition', json.dumps(ludwig_model.model_definition))\n",
    "\n",
    "    # Load the train/validation/test files\n",
    "    data_train_df = read_csv_dataframe(args.train, engine=args.pandas_engine)\n",
    "    data_validation_df = read_csv_dataframe(args.validation, engine=args.pandas_engine)\n",
    "    data_test_df = read_csv_dataframe(args.test, engine=args.pandas_engine)\n",
    "    \n",
    "    print('training model...')\n",
    "    train_stats = ludwig_model.train(\n",
    "        skip_save_log=True, # Don't save tensorboard\n",
    "        skip_save_processed_input=True, # Don't save pre-processed input\n",
    "        data_train_df=data_train_df,\n",
    "        data_validation_df=data_validation_df,\n",
    "        data_test_df=data_test_df,\n",
    "        output_directory=args.output_dir,\n",
    "        experiment_name=args.experiment_name,\n",
    "        model_name=args.trial_name\n",
    "    )\n",
    "\n",
    "    # TODO: Output stats for logging\n",
    "    print('train stats', json.dumps(train_stats))\n",
    "        \n",
    "    # Save the ludwig model \n",
    "    ludwig_model.save(args.model_dir)\n",
    "    \n",
    "#     # Optionally save the model for serving in a numbered directory\n",
    "#     saved_model_path = os.path.join(args.model_dir, str(int(time.time())))\n",
    "#     ludwig_model.save_for_serving(saved_model_path)\n",
    "    \n",
    "    # Print output files and close\n",
    "    print('model output', os.listdir(args.model_dir))\n",
    "    ludwig_model.close()    \n",
    "        \n",
    "ludwig_model = None\n",
    "args = None\n",
    "\n",
    "def load_model():\n",
    "    global ludwig_model\n",
    "    global args\n",
    "    if ludwig_model == None:\n",
    "        # Load model and print definition if not already loaded\n",
    "        args, _ = parse_args()\n",
    "        print('args', args)\n",
    "        print('loading model...')\n",
    "        ludwig_model = LudwigModel.load(args.model_dir)\n",
    "        print('model definition', json.dumps(ludwig_model.model_definition))        \n",
    "    return ludwig_model, args\n",
    "    \n",
    "@get('/ping')\n",
    "def ping():\n",
    "    print('ping')\n",
    "    # Load/cache the model on ping\n",
    "    load_model()\n",
    "    return \"OK\"\n",
    "\n",
    "@post('/invocations')\n",
    "def invoke():\n",
    "    payload = request.body.read().decode('utf-8')\n",
    "    data_df = pd.read_csv(StringIO(payload))\n",
    "    print('invoke')\n",
    "    print(data_df)\n",
    "    ludwig_model, _ = load_model()\n",
    "    predictions = ludwig_model.predict(data_df=data_df)\n",
    "    return predictions.to_csv(index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2 or ( not sys.argv[1] in [ \"serve\", \"train\", \"test\"] ):\n",
    "        raise Exception(\"Invalid argument: you must inform 'train' for training mode or 'serve' predicting mode\") \n",
    "\n",
    "    train = sys.argv[1] == \"train\"\n",
    "    test = sys.argv[1] == \"test\"\n",
    "\n",
    "    # TEMP: Print out all files mounted under /opt/ml\n",
    "    print([os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(\"/opt/ml\")) for f in fn])    \n",
    "    \n",
    "    if train:\n",
    "        ludwig_train()\n",
    "    elif test:\n",
    "        # Read and write to local file\n",
    "        print('test', sys.argv[2], sys.argv[3], sys.argv[4])\n",
    "        data_df = pd.read_csv(sys.argv[2])\n",
    "        print(data_df.head())\n",
    "        ludwig_model, args = load_model()\n",
    "        predictions, test_stats = ludwig_model.test(data_df=data_df)    \n",
    "        predictions.to_csv(sys.argv[3], index=False)\n",
    "        # Write test stats to a file\n",
    "        with open(sys.argv[4], 'w') as tsf:    \n",
    "            json.dump(test_stats, tsf, cls=NpEncoder)        \n",
    "    else:\n",
    "        bjoern.run(bottle.app(), \"0.0.0.0\", 8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finally, let's create the buildspec\n",
    "\n",
    "This file will be used by CodeBuild for creating our base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile buildspec.yml\n",
    "version: 0.2\n",
    "\n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      docker: 18\n",
    "        \n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo Logging in to Amazon ECR...\n",
    "      - $(aws ecr get-login --no-include-email --region us-east-1 --registry-ids 763104351884) \n",
    "      - $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION --registry-ids $AWS_ACCOUNT_ID)\n",
    "  build:\n",
    "    commands:\n",
    "      - echo Build started on `date` for commit $CODEBUILD_RESOLVED_SOURCE_VERSION\n",
    "      - echo Building the Docker image... \n",
    "      - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .\n",
    "      - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo Build completed on `date`\n",
    "      - echo Pushing the Docker image...\n",
    "      - echo docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - echo $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG > image.url\n",
    "      - cat image.url\n",
    "      - echo Done!\n",
    "artifacts:\n",
    "  files:\n",
    "    - image.url\n",
    "  name: image_url\n",
    "  discard-paths: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the image locally, first\n",
    "\n",
    "Test that we can pull the tensorflow training image and then build the local docker.\n",
    "\n",
    "The Dockerfile and buildspec.yaml by default pull the `tensorflow-training` image from `us-east-1` region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$(aws ecr get-login --no-include-email --region us-east-1 --registry-ids 763104351884)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo docker build -f Dockerfile -t scikit-base:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we push our code to the repo, let's check the building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "repo_name='scikit-base'\n",
    "image_tag='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tests\n",
    "!cp app.py Dockerfile buildspec.yml tests/\n",
    "with open('tests/vars.env', 'w') as f:\n",
    "    f.write(\"AWS_ACCOUNT_ID=%s\\n\" % account_id)\n",
    "    f.write(\"IMAGE_TAG=%s\\n\" % image_tag)\n",
    "    f.write(\"IMAGE_REPO_NAME=%s\\n\" % repo_name)\n",
    "    f.write(\"AWS_DEFAULT_REGION=%s\\n\" % region)\n",
    "    f.write(\"AWS_ACCESS_KEY_ID=%s\\n\" % credentials.access_key)\n",
    "    f.write(\"AWS_SECRET_ACCESS_KEY=%s\\n\" % credentials.secret_key)\n",
    "    f.write(\"AWS_SESSION_TOKEN=%s\\n\" % credentials.token )\n",
    "    f.close()\n",
    "\n",
    "!cat tests/vars.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!/tmp/aws-codebuild/local_builds/codebuild_build.sh \\\n",
    "    -a \"$PWD/tests/output\" \\\n",
    "    -s \"$PWD/tests\" \\\n",
    "    -i \"samirsouza/aws-codebuild-standard:2.0\" \\\n",
    "    -e \"$PWD/tests/vars.env\" \\\n",
    "    -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now it's time to push everything to the correct repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../../../mlops-workshop-images/scikit_base\n",
    "cp $OLDPWD/buildspec.yml $OLDPWD/app.py $OLDPWD/Dockerfile .\n",
    "\n",
    "git add buildspec.yml app.py Dockerfile\n",
    "git commit -a -m \" - files for building a scikit learn image\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, now open the AWS console in another tab and go to the CodePipeline console to see the status of our building pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
