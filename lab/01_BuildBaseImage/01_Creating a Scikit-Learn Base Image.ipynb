{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Scikit-Learn base Docker Image\n",
    "\n",
    "We will start our MLOps journey here by creating an abstract Docker Image for supporting [Ludwig](https://uber.github.io/ludwig/) models.\n",
    "\n",
    "So, after we create and test locally our Dockerfile, we'll send it to our first pipeline that will build this image and make it available in ECR.\n",
    "\n",
    "This image will be based on `tensorflow-training`, and install python libraries for serving inference and `ludwig`.\n",
    "\n",
    "## First, lets create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM 763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:1.14-cpu-py3\n",
    "    \n",
    "RUN apt-get update -y && apt-get install -y libev-dev\n",
    "RUN pip install bottle bjoern ludwig\n",
    "\n",
    "RUN mkdir -p /opt/program\n",
    "RUN mkdir -p /opt/ml\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "ENV PATH=\"/opt/program:${PATH}\"\n",
    "\n",
    "COPY app.py /opt/program\n",
    "WORKDIR /opt/program\n",
    "\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, the a basic application that will host our model code\n",
    "\n",
    "Please, notice that we're creating a WebService application with two methods: **ping** and **invocations**. Ping is for healthcheck and invocations is for calling your model.\n",
    "\n",
    "For a production environment it is important to use a **WSGI** solution. We will use a combo of **bottle** and **bjoern**. Bottle is our webservice api and bjoern our WSGI server. Since bjoern is single threaded, you can't run multiple predictions at the same time. If you need something like that, maybe you need gunicorn and a reverse proxy to protect your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Import python serving\n",
    "import bjoern\n",
    "import bottle\n",
    "from bottle import run, request, post, get\n",
    "\n",
    "# Import ludwig library\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import ludwig\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "print('ludwig: {}'.format(ludwig.__version__))\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # parameters for training (TODO: Add horovod etc)\n",
    "    parser.add_argument('--experiment_name', type=str, default='sagemaker_experiment')\n",
    "    parser.add_argument('--trail_name', type=str, default='run')\n",
    "    \n",
    "    # data directories\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN', '/opt/ml/input/data/training'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST', '/opt/ml/input/data/test'))\n",
    "\n",
    "    # input_config\n",
    "    parser.add_argument('--config_dir', type=str, default=os.environ.get('SM_INPUT_CONFIG_DIR', '/opt/ml/input/config'))\n",
    "\n",
    "    # model directory: we will use the default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument('--output_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR', '/opt/ml/output/data'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', '/opt/ml/model'))\n",
    "    \n",
    "    return parser.parse_known_args()   \n",
    "\n",
    "def ludwig_train():\n",
    "    args, _ = parse_args()\n",
    "    print(args)\n",
    "    \n",
    "    # Create model from definition\n",
    "    ludwig_model = LudwigModel(None, model_definition_file='model_definition.yml')\n",
    "    \n",
    "    # Allow specifying training hyperparameters \n",
    "    # see: https://uber.github.io/ludwig/user_guide/#training\n",
    "    trainnig_config_path = os.path.join(args.config_dir, 'hyperparameters.config')\n",
    "    if os.path.exists(trainnig_config_path):\n",
    "        with open(trainnig_config_path, 'r') as tc:\n",
    "            ludwig_model.model_definition['training'] = json.load(tc)\n",
    "\n",
    "    print(json.dumps(ludwig_model.model_definition))\n",
    "\n",
    "    # TODO: Fild all files in train/test folders\n",
    "    train_csv = os.path.join(args.train, 'train.csv')\n",
    "    test_csv = os.path.join(args.test, 'test.csv')\n",
    "    \n",
    "    print('training model...')\n",
    "    train_stats = ludwig_model.train(\n",
    "        skip_save_log=True, # Don't save tensorboard\n",
    "        skip_save_processed_input=True, # Don't save pre-processed input\n",
    "        data_train_csv=train_csv,\n",
    "        data_validation_csv=test_csv,\n",
    "        output_directory=args.output_dir,\n",
    "        experiment_name=args.experiment_name,\n",
    "        model_name=args.trail_name\n",
    "    )\n",
    "\n",
    "    # TODO: Output stats for logging\n",
    "    print(json.dumps(train_stats))\n",
    "        \n",
    "    # Save the ludwig model \n",
    "    ludwig_model.save(args.model_dir)\n",
    "    \n",
    "#     # Optionally save the model for serving in a numbered directory\n",
    "#     saved_model_path = os.path.join(args.model_dir, str(int(time.time())))\n",
    "#     ludwig_model.save_for_serving(saved_model_path)\n",
    "    \n",
    "    # Print output files and close\n",
    "    print(os.listdir(args.model_dir))\n",
    "    ludwig_model.close()\n",
    "\n",
    "ludwig_model = None\n",
    "    \n",
    "def ludwig_test(data_df):\n",
    "    global ludwig_model\n",
    "    if ludwig_model == None:\n",
    "        args, _ = parse_args()\n",
    "        print(args)\n",
    "\n",
    "        # Load model and print definition\n",
    "        print('loading model...')\n",
    "        ludwig_model = LudwigModel.load(args.model_dir)\n",
    "        print(json.dumps(ludwig_model.model_definition))\n",
    "        \n",
    "    return ludwig_model.predict(data_df=data_df)\n",
    "    \n",
    "@get('/ping')\n",
    "def ping():\n",
    "    print('ping')\n",
    "    return \"OK\"\n",
    "\n",
    "@post('/invocations')\n",
    "def invoke():\n",
    "    payload = request.body.read().decode('utf-8')\n",
    "    data_df = pd.read_csv(StringIO(payload))\n",
    "    print('invoke')\n",
    "    print(data_df)\n",
    "    predictions = ludwig_test(data_df)\n",
    "    return predictions.to_csv(index=False)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2 or ( not sys.argv[1] in [ \"serve\", \"train\", \"test\"] ):\n",
    "        raise Exception(\"Invalid argument: you must inform 'train' for training mode or 'serve' predicting mode\") \n",
    "\n",
    "    train = sys.argv[1] == \"train\"\n",
    "    test = sys.argv[1] == \"test\"\n",
    "\n",
    "    # TEMP: Print out all files mounted under /opt/ml\n",
    "    print([os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(\"/opt/ml\")) for f in fn])    \n",
    "    \n",
    "    if train:\n",
    "        ludwig_train()\n",
    "    elif test:\n",
    "        # Read and write to local file\n",
    "        print('test', sys.argv[2], sys.argv[3])\n",
    "        data_df = pd.read_csv(sys.argv[2])\n",
    "        print(data_df.head())\n",
    "        predictions = ludwig_test(data_df)\n",
    "        predictions.to_csv(sys.argv[3], index=False)\n",
    "    else:\n",
    "        bjoern.run(bottle.app(), \"0.0.0.0\", 8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finally, let's create the buildspec\n",
    "\n",
    "This file will be used by CodeBuild for creating our base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile buildspec.yml\n",
    "version: 0.2\n",
    "\n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      docker: 18\n",
    "        \n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo Logging in to Amazon ECR...\n",
    "      - $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
    "  build:\n",
    "    commands:\n",
    "      - echo Build started on `date`\n",
    "      - echo Building the Docker image...\n",
    "      - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .\n",
    "      - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo Build completed on `date`\n",
    "      - echo Pushing the Docker image...\n",
    "      - echo docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - echo $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG > image.url\n",
    "      - echo Done!\n",
    "artifacts:\n",
    "  files:\n",
    "    - image.url\n",
    "  name: image_url\n",
    "  discard-paths: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the image locally, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo docker build -f Dockerfile -t ludwig-base:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we push our code to the repo, let's check the building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "repo_name='ludwig-base'\n",
    "image_tag='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tests\n",
    "!cp app.py Dockerfile buildspec.yml tests/\n",
    "with open('tests/vars.env', 'w') as f:\n",
    "    f.write(\"AWS_ACCOUNT_ID=%s\\n\" % account_id)\n",
    "    f.write(\"IMAGE_TAG=%s\\n\" % image_tag)\n",
    "    f.write(\"IMAGE_REPO_NAME=%s\\n\" % repo_name)\n",
    "    f.write(\"AWS_DEFAULT_REGION=%s\\n\" % region)\n",
    "    f.write(\"AWS_ACCESS_KEY_ID=%s\\n\" % credentials.access_key)\n",
    "    f.write(\"AWS_SECRET_ACCESS_KEY=%s\\n\" % credentials.secret_key)\n",
    "    f.write(\"AWS_SESSION_TOKEN=%s\\n\" % credentials.token )\n",
    "    f.close()\n",
    "\n",
    "!cat tests/vars.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!/tmp/aws-codebuild/local_builds/codebuild_build.sh \\\n",
    "    -a \"$PWD/tests/output\" \\\n",
    "    -s \"$PWD/tests\" \\\n",
    "    -i \"samirsouza/aws-codebuild-standard:2.0\" \\\n",
    "    -e \"$PWD/tests/vars.env\" \\\n",
    "    -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now it's time to push everything to the correct repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../../../mlops-workshop-images/scikit_base\n",
    "cp $OLDPWD/buildspec.yml $OLDPWD/app.py $OLDPWD/Dockerfile .\n",
    "\n",
    "git add buildspec.yml app.py Dockerfile\n",
    "git commit -a -m \" - files for building a scikit learn image\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, now open the AWS console in another tab and go to the CodePipeline console to see the status of our building pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
