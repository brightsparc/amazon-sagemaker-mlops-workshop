{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model Docker Image\n",
    "\n",
    "Now it's time to extend the abstract image we just created for Lugwig algorithms and implement a Concrete Docker Image with our algorithms/models.\n",
    "\n",
    "Here, we'll prepare a Docker image with an algorithm to classify our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_repo = os.environ['BASE_REPO']\n",
    "print('base repo: {}'.format(base_repo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, lets create a Dockerfile\n",
    "\n",
    "Inherit Dockerfile from `$BASE_REPO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Writing Dockerfile')\n",
    "\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write('''FROM {}:latest\n",
    "\n",
    "COPY model_definition.yml /opt/program\n",
    "'''.format(base_repo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, let's the model_definition file\n",
    "\n",
    "Define a model definition file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_definition.yml\n",
    "input_features:\n",
    "    -\n",
    "        name: text\n",
    "        type: text\n",
    "        level: word\n",
    "        encoder: parallel_cnn\n",
    "\n",
    "output_features:\n",
    "    -\n",
    "        name: class\n",
    "        type: category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Finally, let's create the buildspec\n",
    "This file will be used by CodeBuild for creating our base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile buildspec.yml\n",
    "version: 0.2\n",
    "\n",
    "phases:\n",
    "  install:\n",
    "    runtime-versions:\n",
    "      docker: 18\n",
    "\n",
    "  pre_build:\n",
    "    commands:\n",
    "      - echo Logging in to Amazon ECR...\n",
    "      - $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION)\n",
    "      - docker pull $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$BASE_REPO_NAME:$IMAGE_TAG\n",
    "      - docker tag $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$BASE_REPO_NAME:$IMAGE_TAG $BASE_REPO_NAME:$IMAGE_TAG\n",
    "  build:\n",
    "    commands:\n",
    "      - echo Build started on `date`\n",
    "      - echo Building the Docker image...\n",
    "      - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .\n",
    "      - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "  post_build:\n",
    "    commands:\n",
    "      - echo Build completed on `date`\n",
    "      - echo Pushing the Docker image...\n",
    "      - echo docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "      - echo $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG > image.url\n",
    "      - cat image.url\n",
    "      - echo Done\n",
    "artifacts:\n",
    "  files:\n",
    "    - image.url\n",
    "  name: image_url\n",
    "  discard-paths: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the image locally, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker build -f Dockerfile -t $IMAGE_REPO:${IMAGE_TAG:-latest} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do some tests, locally\n",
    "## First, let's define some hyperparameters for both algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Lots apply these to the 'training' parameters section if required\n",
    "hyperparameters = {\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "!mkdir -p input/config\n",
    "\n",
    "hyperparameters = dict({key: str(values) for key, values in hyperparameters.items()})\n",
    "with open('input/config/hyperparameters.json', 'w') as f:\n",
    "    f.write(json.dumps(hyperparameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, let's prepare a dataset\n",
    "\n",
    "Download the input data which should contain `training`, `validation` and optional `test` paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bucket = os.environ.get('DATASET_BUCKET')\n",
    "dataset_prefix = os.environ.get('DATASET_PREFIX')\n",
    "\n",
    "print('dataset: {}/{}'.format(dataset_bucket, dataset_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf input\n",
    "!aws s3 sync  s3://$dataset_bucket/$dataset_prefix input/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, let's test the training process\n",
    "\n",
    "Make model directory and clear any existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model\n",
    "!rm -Rf model/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( \"Training ...\")\n",
    "!docker run --rm --name $IMAGE_REPO-train \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\"  $IMAGE_REPO:${IMAGE_TAG:-latest} train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, a basic test with a direct call to our container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf output\n",
    "!mkdir -p output/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls input/data/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( \"Testing\")\n",
    "!docker run --rm --name $IMAGE_REPO-test \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/output:/opt/ml/output\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" $IMAGE_REPO:${IMAGE_TAG:-latest} test \\\n",
    "        '/opt/ml/input/data/validation' \\\n",
    "        '/opt/ml/output/data/predictions.csv' \\\n",
    "        '/opt/ml/output/data/test_stats.json' \\\n",
    "        --pandas_header=True # output the CSV head for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the overall stats from validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('output/data/test_stats.json', 'r') as f:\n",
    "    test_stats = json.load(f)\n",
    "    \n",
    "test_stats['class']['overall_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the output predictions which we will use for baselining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l output/data/predictions.csv\n",
    "!tail -3 output/data/predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the serving test. It simulates an Endpoint exposed by Sagemaker\n",
    "\n",
    "After you execute the next cell, this Jupyter notebook will freeze. A webservice will be exposed at the port 8080. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm --name $IMAGE_REPO-serve \\\n",
    "    -p 8080:8080 \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD/input:/opt/ml/input\" $IMAGE_REPO:${IMAGE_TAG:-latest} serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While the above cell is running, click here [TEST NOTEBOOK](02_Testing%20our%20local%20model%20server.ipynb) to run some tests.\n",
    "\n",
    "> After you finish the tests, press **STOP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we push our code to the repo, let's check the building process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "session = boto3.session.Session()\n",
    "\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = session.region_name\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "base_repo_name=os.environ['BASE_REPO']\n",
    "repo_name=os.environ['IMAGE_REPO']\n",
    "image_tag='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tests\n",
    "!cp model_definition.yml Dockerfile buildspec.yml tests/\n",
    "with open('tests/vars.env', 'w') as f:\n",
    "    f.write(\"AWS_ACCOUNT_ID=%s\\n\" % account_id)\n",
    "    f.write(\"IMAGE_TAG=%s\\n\" % image_tag)\n",
    "    f.write(\"BASE_REPO_NAME=%s\\n\" % base_repo_name)\n",
    "    f.write(\"IMAGE_REPO_NAME=%s\\n\" % repo_name)\n",
    "    f.write(\"AWS_DEFAULT_REGION=%s\\n\" % region)\n",
    "    f.write(\"AWS_ACCESS_KEY_ID=%s\\n\" % credentials.access_key)\n",
    "    f.write(\"AWS_SECRET_ACCESS_KEY=%s\\n\" % credentials.secret_key)\n",
    "    f.write(\"AWS_SESSION_TOKEN=%s\\n\" % credentials.token )\n",
    "    f.close()\n",
    "\n",
    "!cat tests/vars.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!/tmp/aws-codebuild/local_builds/codebuild_build.sh \\\n",
    "    -a \"$PWD/tests/output\" \\\n",
    "    -s \"$PWD/tests\" \\\n",
    "    -i \"samirsouza/aws-codebuild-standard:2.0\" \\\n",
    "    -e \"$PWD/tests/vars.env\" \\\n",
    "    -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now it's time to push everything to the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../../../mlops-workshop-images/$IMAGE_REPO\n",
    "cp $OLDPWD/buildspec.yml $OLDPWD/model_definition.yml $OLDPWD/Dockerfile .\n",
    "\n",
    "git add --all\n",
    "git commit -a -m \" - files for building $IMAGE_REPO image\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, now open the AWS console in another tab and go to the CodePipeline console to see the status of our building pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
