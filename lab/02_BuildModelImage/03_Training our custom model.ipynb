{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we can start a new training job\n",
    "\n",
    "We'll send a zip file called **trainingjob.zip**, with the following structure:\n",
    " - trainingjob.json (Sagemaker training job descriptor)\n",
    " - monitoring.json (Sagemaker baseline input and monitoring schedule params)\n",
    " - assets/deploy-model-prd.yml (Cloudformation for deploying our model into Production)\n",
    " - assets/deploy-model-dev.yml (Cloudformation for deploying our model into Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "artifact_bucket = os.environ['ARTIFACT_BUCKET']\n",
    "prefix = os.environ['MODEL_NAME']\n",
    "image_repo = os.environ['IMAGE_REPO']\n",
    "\n",
    "print('artifact bucket: {}'.format(artifact_bucket))\n",
    "print('image repo: {}'.format(image_repo))\n",
    "print('data bucket: {}/{}'.format(bucket, prefix))\n",
    "print('role: {}'.format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Upload training data\n",
    "\n",
    "Validate and upload the training and validation datasets to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -R input/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_uri = sagemaker_session.upload_data(path='input/data/training', key_prefix=prefix+'/input/training')\n",
    "validation_uri = sagemaker_session.upload_data(path='input/data/validation', key_prefix=prefix+'/input/validation')\n",
    "output_uri = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "print('Training uri: {}'.format(training_uri))\n",
    "print('Validation uri: {}'.format(validation_uri))\n",
    "print('Model output uri: {}'.format(output_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training job decriptor\n",
    "\n",
    "This includes some hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training job image, and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = boto3.session.Session().region_name\n",
    "training_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, image_repo)\n",
    "\n",
    "# Set the job name which will get override in code pipeline \n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = prefix + timestamp\n",
    "\n",
    "training_params = {}\n",
    "\n",
    "# Here we set the reference for the Image Classification Docker image, stored on ECR (https://aws.amazon.com/pt/ecr/)\n",
    "training_params[\"AlgorithmSpecification\"] = {\n",
    "    \"TrainingImage\": training_image,\n",
    "    \"TrainingInputMode\": \"File\",\n",
    "    \"MetricDefinitions\": [\n",
    "        {'Name':'train:loss', 'Regex':'Train Loss: (.*?);'},\n",
    "        {'Name':'train:accuracy', 'Regex':'Train Accuracy: (.*?)%;'},\n",
    "        {'Name':'val:loss', 'Regex':'Validation Loss: (.*?);'},\n",
    "        {'Name':'val:accuracy', 'Regex':'Validation Accuracy: (.*?)%;'},\n",
    "        {'Name':'test:loss', 'Regex':'Test Loss: (.*?);'},\n",
    "        {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# The IAM role with all the permissions given to Sagemaker\n",
    "training_params[\"RoleArn\"] = role\n",
    "\n",
    "# Here Sagemaker will store the final trained model\n",
    "training_params[\"OutputDataConfig\"] = {\n",
    "    \"S3OutputPath\": output_uri\n",
    "}\n",
    "\n",
    "# This is the config of the instance that will execute the training\n",
    "training_params[\"ResourceConfig\"] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "    \"VolumeSizeInGB\": 30\n",
    "}\n",
    "\n",
    "# The job name. You'll see this name in the Jobs section of the Sagemaker's console\n",
    "training_params[\"TrainingJobName\"] = job_name\n",
    "\n",
    "for i in hyperparameters:\n",
    "    hyperparameters[i] = str(hyperparameters[i])\n",
    "    \n",
    "# Here you will configure the hyperparameters used for training your model.\n",
    "training_params[\"HyperParameters\"] = hyperparameters\n",
    "\n",
    "# Training timeout\n",
    "training_params[\"StoppingCondition\"] = {\n",
    "    \"MaxRuntimeInSeconds\": 360000\n",
    "}\n",
    "\n",
    "# The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "training_params[\"InputDataConfig\"] = [{\n",
    "    \"ChannelName\": \"training\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": training_uri,\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\"\n",
    "},{\n",
    "    \"ChannelName\": \"validation\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": validation_uri,\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\"\n",
    "}]\n",
    "\n",
    "training_params[\"Tags\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload baseline data\n",
    "\n",
    "Validate, and upload the baseline input to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output predictions (NOTE: if using scientific format these will be treated as strings)\n",
    "baseline_file = 'output/data/predictions.csv'\n",
    "\n",
    "!head -2 $baseline_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_uri = sagemaker_session.upload_data(path=baseline_file, key_prefix=prefix+'/input/baseline')\n",
    "\n",
    "print('Baseline uri: {}'.format(baseline_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training job hash so we can force update of deployment.\n",
    "\n",
    "Until AutoPublishCodeSha256 support to force Lambda redployment [see PR](https://github.com/awslabs/serverless-application-model/pull/1376) we need to update the lambda zip contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "training_hash = hashlib.sha256(json.dumps(training_params).encode('utf-8')).hexdigest()\n",
    "print('training hash: {}'.format(training_hash))\n",
    "\n",
    "# TEMP: Write a new file to the API directory to force refresh\n",
    "with open('../../api/training_hash.txt', 'w') as f:\n",
    "    f.write(training_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the training job and monitoring json files as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_params = {\n",
    "    'TrainSha256': training_hash,\n",
    "    'BaselineInputUri': baseline_uri,\n",
    "    'ScheduleMetricName': 'feature_baseline_drift_class_predictions', # alarm on class predictions drift\n",
    "    'ScheduleMetricThreshold': str(0.4) # Must serialize parameters as string\n",
    "}\n",
    "\n",
    "with open('trainingjob.json', 'w') as f:\n",
    "    json.dump(training_params, f)\n",
    "with open('monitoring.json', 'w') as f:\n",
    "    json.dump(monitoring_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload deployment artifacts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the cloud formation template with API serverless endpoints uploading code to sagemaker bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws cloudformation package --template-file ../../assets/deploy-model-prd.yml \\\n",
    "    --output-template-file ../../assets/template-model-prd.yml --s3-bucket $artifact_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the template has been generated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../../assets/template-model-prd.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now it's time to push everything to the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../../../mlops-workshop-images/master\n",
    "mkdir -p assets\n",
    "\n",
    "cp $OLDPWD/trainingjob.json $OLDPWD/monitoring.json .\n",
    "cp ../../mlops-workshop/assets/template-model-prd.yml assets/deploy-model-prd.yml  # Save as original name\n",
    "cp ../../mlops-workshop/assets/deploy-model-dev.yml assets/deploy-model-dev.yml\n",
    "cp ../../mlops-workshop/assets/wait-training-job.yml assets/wait-training-job.yml\n",
    "\n",
    "git add --all\n",
    "git commit -a -m \" - test updated deployment\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, now open the AWS console in another tab and go to the CodePipeline console to see the status of our building pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally, click here [NOTEBOOK](04_Check%20Progress%20and%20Test%20the%20endpoint.ipynb) to see the progress and test your endpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
