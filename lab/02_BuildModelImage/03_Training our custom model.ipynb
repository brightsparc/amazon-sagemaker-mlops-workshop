{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we can start a new training job\n",
    "\n",
    "We'll send a zip file called **trainingjob.zip**, with the following structure:\n",
    " - trainingjob.json (Sagemaker training job descriptor)\n",
    " - monitoring.json (Sagemaker monitoring inputs for data capture, baseline and schedule)\n",
    " - assets/deploy-model-prd.yml (Cloudformation for deploying our model into Production)\n",
    " - assets/deploy-model-dev.yml (Cloudformation for deploying our model into Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "artifact_bucket = os.environ['ARTIFACT_BUCKET']\n",
    "prefix = os.environ['MODEL_NAME']\n",
    "image_repo = os.environ['IMAGE_REPO']\n",
    "\n",
    "print('artifact bucket: {}'.format(artifact_bucket))\n",
    "print('image repo: {}'.format(image_repo))\n",
    "print('data bucket: {}/{}'.format(bucket, prefix))\n",
    "print('role: {}'.format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training job decriptor\n",
    "\n",
    "This includes some hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training job image, and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = boto3.session.Session().region_name\n",
    "training_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, image_repo)\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = prefix + timestamp\n",
    "\n",
    "training_params = {}\n",
    "\n",
    "# Here we set the reference for the Image Classification Docker image, stored on ECR (https://aws.amazon.com/pt/ecr/)\n",
    "training_params[\"AlgorithmSpecification\"] = {\n",
    "    \"TrainingImage\": training_image,\n",
    "    \"TrainingInputMode\": \"File\",\n",
    "    \"MetricDefinitions\": [\n",
    "        {'Name':'train:loss', 'Regex':'Train Loss: (.*?);'},\n",
    "        {'Name':'train:accuracy', 'Regex':'Train Accuracy: (.*?)%;'},\n",
    "        {'Name':'val:loss', 'Regex':'Validation Loss: (.*?);'},\n",
    "        {'Name':'val:accuracy', 'Regex':'Validation Accuracy: (.*?)%;'},\n",
    "        {'Name':'test:loss', 'Regex':'Test Loss: (.*?);'},\n",
    "        {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# The IAM role with all the permissions given to Sagemaker\n",
    "training_params[\"RoleArn\"] = role\n",
    "\n",
    "# Here Sagemaker will store the final trained model\n",
    "training_params[\"OutputDataConfig\"] = {\n",
    "    \"S3OutputPath\": 's3://{}/{}'.format(bucket, prefix)\n",
    "}\n",
    "\n",
    "# This is the config of the instance that will execute the training\n",
    "training_params[\"ResourceConfig\"] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "    \"VolumeSizeInGB\": 30\n",
    "}\n",
    "\n",
    "# The job name. You'll see this name in the Jobs section of the Sagemaker's console\n",
    "training_params[\"TrainingJobName\"] = job_name\n",
    "\n",
    "for i in hyperparameters:\n",
    "    hyperparameters[i] = str(hyperparameters[i])\n",
    "    \n",
    "# Here you will configure the hyperparameters used for training your model.\n",
    "training_params[\"HyperParameters\"] = hyperparameters\n",
    "\n",
    "# Training timeout\n",
    "training_params[\"StoppingCondition\"] = {\n",
    "    \"MaxRuntimeInSeconds\": 360000\n",
    "}\n",
    "\n",
    "# The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "training_params[\"InputDataConfig\"] = [{\n",
    "    \"ChannelName\": \"training\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/input/training'.format(bucket, prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\"\n",
    "},{\n",
    "    \"ChannelName\": \"validation\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/input/validation'.format(bucket, prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\"\n",
    "}]\n",
    "training_params[\"Tags\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Upload training data\n",
    "\n",
    "Validate the training / test sets and upload these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc = sagemaker_session.upload_data(path='input/data/training', key_prefix=prefix+'/input/training')\n",
    "val_loc = sagemaker_session.upload_data(path='input/data/validation', key_prefix=prefix+'/input/validation')\n",
    "\n",
    "print('training: {}'.format(train_loc))\n",
    "print('validation: {}'.format(val_loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure monitoring inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data capture config for endpoints\n",
    "\n",
    "1. Data Capture log output\n",
    "2. Baseline input location with file uploaded to s3\n",
    "3. Baseline results s3 location\n",
    "4. Schedule resports s3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_capture_uri = 's3://{}/{}/datacapture'.format(bucket, prefix)\n",
    "print('data capture uri: {}'.format(data_capture_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the output predictions from testing for baseline file.  Make sure we have headers on this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output predictions (NOTE: if using scientific format these will be treated as strings)\n",
    "baseline_file = 'output/data/predictions.csv'\n",
    "!head -2 $baseline_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the predictions as baseline file\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(baseline_file).upload_file(baseline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_file)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print('Baseline data file: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the location for the monitor schedule outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_reports_uri = 's3://{}/{}/monitoring/reports'.format(bucket, prefix)\n",
    "\n",
    "print('monitoring reports: {}'.format(monitoring_reports_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training job hash so we can force update of deployment.\n",
    "\n",
    "Until AutoPublishCodeSha256 support to force Lambda redployment [see PR](https://github.com/awslabs/serverless-application-model/pull/1376) we need to update the lambda zip contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "training_hash = hashlib.sha256(json.dumps(training_params).encode('utf-8')).hexdigest()\n",
    "print('training hash: {}'.format(training_hash))\n",
    "\n",
    "# TEMP: Write a new file to the API directory to force refresh\n",
    "with open('../../api/training_hash.txt', 'w') as f:\n",
    "    f.write(training_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the training job and monitoring json files as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_params = {\n",
    "    'TrainSha256': training_hash,\n",
    "    'DataCaptureUri': data_capture_uri,\n",
    "    'MonitoringRoleArn': role,\n",
    "    'BaselineInputUri': baseline_data_uri,\n",
    "    'BaselineResultsUri':  baseline_results_uri,\n",
    "    'ScheduleReportsUri': monitoring_reports_uri,\n",
    "    'ScheduleMetricName': 'feature_baseline_drift_class_predictions', # alarm on class predictions drift\n",
    "    'ScheduleMetricThreshold': str(0.4) # Must serialize parameters as string\n",
    "}\n",
    "\n",
    "with open('trainingjob.json', 'w') as f:\n",
    "    json.dump(training_params, f)\n",
    "with open('monitoring.json', 'w') as f:\n",
    "    json.dump(monitoring_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload deployment artifacts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the cloud formation template with API serverless endpoints uploading code to sagemaker bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws cloudformation package --template-file ../../assets/deploy-model-prd.yml \\\n",
    "    --output-template-file ../../assets/template-model-prd.yml --s3-bucket $artifact_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the template has been generated correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../../assets/template-model-prd.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now it's time to push everything to the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../../../mlops-workshop-images/master\n",
    "mkdir -p assets\n",
    "\n",
    "cp $OLDPWD/trainingjob.json $OLDPWD/monitoring.json .\n",
    "cp ../../mlops-workshop/assets/template-model-prd.yml assets/deploy-model-prd.yml  # Save as original name\n",
    "cp ../../mlops-workshop/assets/deploy-model-dev.yml assets/deploy-model-dev.yml\n",
    "cp ../../mlops-workshop/assets/wait-training-job.yml assets/wait-training-job.yml\n",
    "\n",
    "git add --all\n",
    "git commit -a -m \" - test updated deployment\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, now open the AWS console in another tab and go to the CodePipeline console to see the status of our building pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally, click here [NOTEBOOK](04_Check%20Progress%20and%20Test%20the%20endpoint.ipynb) to see the progress and test your endpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
