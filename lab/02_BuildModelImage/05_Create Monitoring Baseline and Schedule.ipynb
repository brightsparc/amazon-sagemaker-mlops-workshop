{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Baseline and Schedule\n",
    "\n",
    "This notebook will take you through the steps\n",
    "1. Enable real-time inference data capture\n",
    "2. Model Monitor - Baseling\n",
    "3. Analyse initial monitoring schedule\n",
    "4. Create monitoring schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Enable real-time inference data capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture for monitoring the model data quality, you specify the new capture option called `DataCaptureConfig`. \n",
    "\n",
    "You can capture the request payload, the response payload or both with this configuration. The capture config applies to all variants. Please provide the Endpoint name in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker import session\n",
    "import boto3\n",
    "\n",
    "sm_session = session.Session(boto3.Session())\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix='text-multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codepipeline = boto3.client('codepipeline')\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "pipeline_name = 'mlops1-text-multiclass'\n",
    "training_job_name_mask='mlops1-text-multiclass-%s'\n",
    "endpoint_name_mask='mlops1-text-multiclass-%s-%s'\n",
    "\n",
    "# Get the current execution id for the latest succesful prod deploy\n",
    "response = codepipeline.get_pipeline_state( name=pipeline_name )\n",
    "executionId = response['stageStates'][-1]['latestExecution']['pipelineExecutionId']\n",
    "endpoint_name = endpoint_name_mask % ('prd', executionId)\n",
    "print('endpoint name: {}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, s3_capture_prefix)\n",
    "print('data capture: {}'.format(s3_capture_upload_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker import session\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "import boto3\n",
    "sm_session = session.Session(boto3.Session())\n",
    "\n",
    "# Change parameters as you would like - adjust sampling percentage, \n",
    "#  chose to capture request or response or both.\n",
    "#  Learn more from our documentation\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture = True,\n",
    "                        sampling_percentage=100,\n",
    "                        destination_s3_uri=s3_capture_upload_path,\n",
    "                        kms_key_id=None,\n",
    "                        capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "                        csv_content_types=[\"text/csv\"],\n",
    "                        json_content_types=[\"application/json\"])\n",
    "\n",
    "# NOTE: The following doesn't work when created by CFN\n",
    "# # Now it is time to apply the new configuration and wait for it to be applied\n",
    "# predictor = RealTimePredictor(endpoint=endpoint_name)\n",
    "# predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "# sm_session.wait_for_endpoint(endpoint=endpoint_name)\n",
    "\n",
    "endpoint = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "if endpoint['EndpointStatus'] != 'InService':\n",
    "    raise(Exception('Endpoint not InService'))\n",
    "\n",
    "# Get the current endpoint config\n",
    "endpoint_config_name = endpoint['EndpointConfigName']\n",
    "new_config_name = name_from_base(base=endpoint_config_name)\n",
    "\n",
    "# Create a new config from the existing adding data capture\n",
    "new_tags = [{'Key': 'datacapture', 'Value': 'true'}] \n",
    "sm_session.create_endpoint_config_from_existing(\n",
    "    endpoint_config_name, new_config_name, new_tags=new_tags, \n",
    "    new_data_capture_config_dict=data_capture_config._to_request_dict())\n",
    "\n",
    "# Update the endpoint\n",
    "sm_session.update_endpoint(endpoint_name=endpoint_name, endpoint_config_name=new_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Monitor - Baseling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to collecting the data, SageMaker allows you to monitor and evaluate the data observed by the Endpoints. \n",
    "\n",
    "For this :\n",
    "1. We need to create a baseline with which we compare the realtime traffic against. \n",
    "1. Once a baseline is ready, we can setup a schedule to continously evaluate/compare against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_file = 'output/data/predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 $baseline_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint suggestion with baseline/training dataset\n",
    "\n",
    "Use the output predictions from test dataset to upload as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_file)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print('Baseline data file: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(baseline_file).upload_file(baseline_file)\n",
    "print('Uploaded baseline: {}'.format(baseline_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a baselining job with the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training data ready in S3, let's kick off a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` kicks off a `ProcessingJob` using a SageMaker provided Model Monitor container to generate the constraints. Please edit the configurations to fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to enable monitoring, you could chose to edit the constraint file as required to fine tune the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyse initial monitoring schedule\n",
    "\n",
    "We have collected the data above, here we proceed to analyze and monitor the data with MonitoringSchedules.\n",
    "\n",
    "Start with sending some different data so that we can then process the data capture in monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Captured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "\n",
    "# Get capture files for this new endpoint\n",
    "results_prefix = s3_capture_prefix+'/'+endpoint_name\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=results_prefix)\n",
    "if not 'Contents' in result:\n",
    "    raise(Exception('No results vailable yet for location: {}'.format(results_prefix)))\n",
    "else:\n",
    "    capture_files = ['s3://{0}/{1}'.format(bucket, capture_file.get(\"Key\")) \n",
    "                     for capture_file in result.get('Contents')][::-1]\n",
    "    print(\"Captured Files: {}, top 3:\".format(len(capture_files)))\n",
    "    print(\"\\n \".join(capture_files[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p baselining/output\n",
    "!aws s3 cp {capture_files[1]} baselining/output/captured_data_example.jsonl\n",
    "!head -1 baselining/output/captured_data_example.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the first payload from this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_event_output(data):\n",
    "    import csv\n",
    "    from io import StringIO\n",
    "    cols = ['class_predictions',\n",
    "             'class_probabilities_<UNK>',\n",
    "             'class_probabilities___label__eating_out',\n",
    "             'class_probabilities___label__groceries',\n",
    "             'class_probabilities___label__transport',\n",
    "             'class_probabilities___label__shopping',\n",
    "             'class_probabilities___label__health',\n",
    "             'class_probabilities___label__travel',\n",
    "             'class_probabilities___label__entertainment',\n",
    "             'class_probabilities___label__education',\n",
    "             'class_probabilities___label__home',\n",
    "             'class_probabilities___label__utilities',\n",
    "             'class_probability']\n",
    "    for row in csv.DictReader(StringIO(data), fieldnames=cols):\n",
    "        return dict(row) # Return first row only, or return list of dicts?\n",
    "\n",
    "with open('baselining/output/captured_data_example.jsonl', 'r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    event = json.loads(lines[0])\n",
    "    print('input: {}\\n{}'.format(event['captureData']['endpointInput']['observedContentType'], \n",
    "                                 event['captureData']['endpointInput']['data'][:200]))\n",
    "    print('output: {}\\n{}'.format(event['captureData']['endpointOutput']['observedContentType'], \n",
    "                                  parse_event_output(event['captureData']['endpointOutput']['data'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an immediate schedule\n",
    "\n",
    "Lets start by running a schedule on some drifted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from urllib.parse import urlparse\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "def get_model_monitor_container_uri(region):\n",
    "    container_uri_format = '{0}.dkr.ecr.{1}.amazonaws.com/sagemaker-model-monitor-analyzer'\n",
    "    \n",
    "    regions_to_accounts = {\n",
    "        'eu-north-1': '895015795356',\n",
    "        'me-south-1': '607024016150',\n",
    "        'ap-south-1': '126357580389',\n",
    "        'us-east-2': '680080141114',\n",
    "        'us-east-2': '777275614652',\n",
    "        'eu-west-1': '468650794304',\n",
    "        'eu-central-1': '048819808253',\n",
    "        'sa-east-1': '539772159869',\n",
    "        'ap-east-1': '001633400207',\n",
    "        'us-east-1': '156813124566',\n",
    "        'ap-northeast-2': '709848358524',\n",
    "        'eu-west-2': '749857270468',\n",
    "        'ap-northeast-1': '574779866223',\n",
    "        'us-west-2': '159807026194',\n",
    "        'us-west-1': '890145073186',\n",
    "        'ap-southeast-1': '245545462676',\n",
    "        'ap-southeast-2': '563025443158',\n",
    "        'ca-central-1': '536280801234'\n",
    "    }\n",
    "    \n",
    "    container_uri = container_uri_format.format(regions_to_accounts[region], region)\n",
    "    return container_uri\n",
    "\n",
    "def get_file_name(url):\n",
    "    a = urlparse(url)\n",
    "    return os.path.basename(a.path)\n",
    "\n",
    "def run_model_monitor_job_processor(region, instance_type, role, data_capture_path, statistics_path, constraints_path, reports_path,\n",
    "                                    instance_count=1, preprocessor_path=None, postprocessor_path=None, publish_cloudwatch_metrics='Disabled'):\n",
    "    \n",
    "    data_capture_sub_path = data_capture_path[data_capture_path.rfind('datacapture/') :]\n",
    "    data_capture_sub_path = data_capture_sub_path[data_capture_sub_path.find('/') + 1 :]\n",
    "    processing_output_paths = reports_path + '/' + data_capture_sub_path\n",
    "    \n",
    "    input_1 = ProcessingInput(input_name='input_1',\n",
    "                          source=data_capture_path,\n",
    "                          destination='/opt/ml/processing/input/endpoint/' + data_capture_sub_path,\n",
    "                          s3_data_type='S3Prefix',\n",
    "                          s3_input_mode='File')\n",
    "\n",
    "    baseline = ProcessingInput(input_name='baseline',\n",
    "                               source=statistics_path,\n",
    "                               destination='/opt/ml/processing/baseline/stats',\n",
    "                               s3_data_type='S3Prefix',\n",
    "                               s3_input_mode='File')\n",
    "\n",
    "    constraints = ProcessingInput(input_name='constraints',\n",
    "                                  source=constraints_path,\n",
    "                                  destination='/opt/ml/processing/baseline/constraints',\n",
    "                                  s3_data_type='S3Prefix',\n",
    "                                  s3_input_mode='File')\n",
    "\n",
    "    outputs = ProcessingOutput(output_name='result',\n",
    "                               source='/opt/ml/processing/output',\n",
    "                               destination=processing_output_paths,\n",
    "                               s3_upload_mode='Continuous')\n",
    "\n",
    "    env = {'baseline_constraints': '/opt/ml/processing/baseline/constraints/' + get_file_name(constraints_path),\n",
    "           'baseline_statistics': '/opt/ml/processing/baseline/stats/' + get_file_name(statistics_path),\n",
    "           'dataset_format': '{\"sagemakerCaptureJson\":{\"captureIndexNames\":[\"endpointInput\",\"endpointOutput\"]}}',\n",
    "           'dataset_source': '/opt/ml/processing/input/endpoint',\n",
    "           'output_path': '/opt/ml/processing/output',\n",
    "           'publish_cloudwatch_metrics': publish_cloudwatch_metrics }\n",
    "    \n",
    "    inputs=[input_1, baseline, constraints]\n",
    "    \n",
    "    if postprocessor_path:\n",
    "        env['post_analytics_processor_script'] = '/opt/ml/processing/code/postprocessing/' + get_file_name(postprocessor_path)\n",
    "        \n",
    "        post_processor_script = ProcessingInput(input_name='post_processor_script',\n",
    "                                                source=postprocessor_path,\n",
    "                                                destination='/opt/ml/processing/code/postprocessing',\n",
    "                                                s3_data_type='S3Prefix',\n",
    "                                                s3_input_mode='File')\n",
    "        inputs.append(post_processor_script)\n",
    "\n",
    "    if preprocessor_path:\n",
    "        env['record_preprocessor_script'] = '/opt/ml/processing/code/preprocessing/' + get_file_name(preprocessor_path)\n",
    "         \n",
    "        pre_processor_script = ProcessingInput(input_name='pre_processor_script',\n",
    "                                               source=preprocessor_path,\n",
    "                                               destination='/opt/ml/processing/code/preprocessing',\n",
    "                                               s3_data_type='S3Prefix',\n",
    "                                               s3_input_mode='File')\n",
    "        \n",
    "        inputs.append(pre_processor_script) \n",
    "    \n",
    "    processor = Processor(image_uri = get_model_monitor_container_uri(region),\n",
    "                          instance_count = instance_count,\n",
    "                          instance_type = instance_type,\n",
    "                          role=role,\n",
    "                          env = env)\n",
    "    \n",
    "    return processor.run(inputs=inputs, outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessor.py\n",
    "import json \n",
    "\n",
    "import json\n",
    "\n",
    "def parse_event_output(data):\n",
    "    import csv\n",
    "    from io import StringIO\n",
    "    cols = ['class_predictions',\n",
    "             'class_probabilities_<UNK>',\n",
    "             'class_probabilities___label__eating_out',\n",
    "             'class_probabilities___label__groceries',\n",
    "             'class_probabilities___label__transport',\n",
    "             'class_probabilities___label__shopping',\n",
    "             'class_probabilities___label__health',\n",
    "             'class_probabilities___label__travel',\n",
    "             'class_probabilities___label__entertainment',\n",
    "             'class_probabilities___label__education',\n",
    "             'class_probabilities___label__home',\n",
    "             'class_probabilities___label__utilities',\n",
    "             'class_probability'] # Define columns\n",
    "    for row in csv.DictReader(StringIO(data), fieldnames=cols):\n",
    "        return dict(row) # Return first row only, or is a list supported\n",
    "\n",
    "def preprocess_handler(inference_record):\n",
    "    try:\n",
    "        # Parse the CSV with header\n",
    "        data = inference_record.endpoint_output.data\n",
    "        if inference_record.endpoint_output.encoding == 'CSV':\n",
    "            data = parse_event_output(data)\n",
    "        return data\n",
    "    except:\n",
    "        # Return an undefined label\n",
    "        return {'class_predictions': '__label__undefined', 'class_probabilities_<UNK>': 1.0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile postprocessor.py\n",
    "def postprocess_handler():\n",
    "    print(\"Hello from post-proc script!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "monitoring_code_prefix = '{0}/monitoring/code'.format(prefix)\n",
    "print(monitoring_code_prefix)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(monitoring_code_prefix + '/preprocessor.py').upload_file('preprocessor.py')\n",
    "s3_preprocessor_path = 's3://{0}/{1}/monitoring/code/preprocessor.py'.format(bucket, prefix)\n",
    "print(s3_preprocessor_path)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(monitoring_code_prefix + '/postprocessor.py').upload_file('postprocessor.py')\n",
    "s3_postprocessor_path = 's3://{0}/{1}/monitoring/code/postprocessor.py'.format(bucket, prefix)\n",
    "print(s3_postprocessor_path)\n",
    "\n",
    "s3_reports_path = 's3://{0}/{1}/monitoring/reports'.format(bucket, prefix)\n",
    "print(s3_reports_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the last statistics/contstraints from capture files\n",
    "s3_data_capture_path = capture_files[len(capture_files) - 1][: capture_files[len(capture_files) - 1].rfind('/')]\n",
    "s3_statistics_path = baseline_results_uri + '/statistics.json'\n",
    "s3_constraints_path = baseline_results_uri + '/constraints.json'\n",
    "\n",
    "print(s3_data_capture_path)\n",
    "print(s3_postprocessor_path)\n",
    "print(s3_statistics_path)\n",
    "print(s3_constraints_path)\n",
    "print(s3_reports_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "processor = run_model_monitor_job_processor(region, 'ml.m5.xlarge', role, \n",
    "                                s3_data_capture_path, s3_statistics_path, s3_constraints_path, s3_reports_path,\n",
    "                                #preprocessor_path=s3_preprocessor_path,\n",
    "                                postprocessor_path=s3_postprocessor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "When the monitoring job completes, monitoring reports are saved to Amazon S3. Let's list the generated reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "monitoring_reports_prefix = '{}/monitoring/reports/{}'.format(prefix, endpoint_name)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=monitoring_reports_prefix)\n",
    "try:\n",
    "    monitoring_reports = ['s3://{0}/{1}'.format(bucket, capture_file.get(\"Key\")) for capture_file in result.get('Contents')]\n",
    "    print(\"Monitoring Reports Files: \")\n",
    "    print(\"\\n \".join(monitoring_reports))\n",
    "except:\n",
    "    print('No monitoring reports found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy monitoring reports locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {monitoring_reports[0]} monitoring/\n",
    "!aws s3 cp {monitoring_reports[1]} monitoring/\n",
    "!aws s3 cp {monitoring_reports[2]} monitoring/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "file = open('monitoring/constraint_violations.json', 'r')\n",
    "data = file.read()\n",
    "\n",
    "violations_df = pd.io.json.json_normalize(json.loads(data)['violations'])\n",
    "violations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Hints\n",
    "\n",
    "You might be asking yourself what are the type of violations that are monitored and how drift from the baseline is computed.\n",
    "\n",
    "The types of violations monitored are listed here: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-violations.html. Most of them use configurable thresholds, that are specified in the monitoring configuration section of the baseline constraints JSON. Let's take a look at this configuration from the baseline constraints file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {statistics_path} baseline/\n",
    "!aws s3 cp {constraints_path} baseline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"baseline/constraints.json\", \"r\") as myfile:\n",
    "    data=myfile.read()\n",
    "\n",
    "print(json.dumps(json.loads(data)['monitoring_config'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration is intepreted when the monitoring job is executed and used to compare captured data to the baseline. If you want to customize this section, you will have to update the constraints.json file and upload it back to Amazon S3 before launching the monitoring job.\n",
    "\n",
    "When data distributions are compared to detect potential drift, you can choose to use either a Simple or Robust comparison method, where the latter has to be preferred when dealing with small datasets. Additional info: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Schedule status: {}'.format(my_default_monitor.describe_schedule()['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Executions\n",
    "\n",
    "The schedule starts jobs at the previously specified intervals. Here, you list the latest five executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait until you cross the hour boundary (in UTC) to see executions kick off. The code below has the logic for waiting.\n",
    "\n",
    "Note: Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start in anywhere from zero to ~20 minutes from the hour boundary. This is expected and done for load balancing in the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\"We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer).\")\n",
    "print(\"We will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    time.sleep(30)\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a specific execution (latest execution)\n",
    "In the previous cell, you picked up the latest completed or failed scheduled execution. Here are the possible terminal states and what each of them mean: \n",
    "* Completed - This means the monitoring execution completed and no issues were found in the violations report.\n",
    "* CompletedWithViolations - This means the execution completed, but constraint violations were detected.\n",
    "* Failed - The monitoring execution failed, maybe due to client error (perhaps incorrect role premissions) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened.\n",
    "* Stopped - job exceeded max runtime or was manually stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "\n",
    "# get the latest completed schedule\n",
    "for execution in mon_executions[::-1]:\n",
    "    latest_job = execution.describe()\n",
    "    print('{:%Y-%m-%d %H:%M} {}\\n{}'.format(latest_job['ProcessingEndTime'], \n",
    "                                            latest_job['ProcessingJobStatus'],\n",
    "                                            latest_job['ProcessingJobArn']))\n",
    "    if latest_job['ProcessingJobStatus'] == 'Completed':\n",
    "        break\n",
    "    time.sleep(1)\n",
    "           \n",
    "if latest_job['ProcessingJobStatus'] == 'Completed':\n",
    "    execution.wait(logs=False)\n",
    "    print(\"Latest execution result: {}\".format(latest_job['ExitMessage']))\n",
    "else:\n",
    "    print(\"====STOP====\\nNo completed executions to inspect further. \\nPlease wait till an execution completes or investigate previously reported failures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import session\n",
    "from sagemaker.model_monitor import MonitoringExecution\n",
    "from sagemaker.s3 import S3Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O utils.py https://raw.githubusercontent.com/awslabs/amazon-sagemaker-examples/master/sagemaker_model_monitor/visualization/utils.py\n",
    "\n",
    "import utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()['ExitMessage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_inputs = {inp['InputName']: inp for inp in execution.describe()['ProcessingInputs']}\n",
    "exec_results = execution.output.destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_statistics_filepath = exec_inputs['baseline']['S3Input']['S3Uri'] if 'baseline' in exec_inputs else None\n",
    "execution_statistics_filepath = os.path.join(exec_results, 'statistics.json')\n",
    "violations_filepath = os.path.join(exec_results, 'constraint_violations.json')\n",
    "\n",
    "baseline_statistics = json.loads(S3Downloader.read_file(baseline_statistics_filepath)) if baseline_statistics_filepath is not None else None\n",
    "execution_statistics = json.loads(S3Downloader.read_file(execution_statistics_filepath))\n",
    "violations = json.loads(S3Downloader.read_file(violations_filepath))['violations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The code below shows the violations and constraichecks across all features in a simple table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.show_violation_df(baseline_statistics=baseline_statistics, latest_statistics=execution_statistics, violations=violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "\n",
    "This section visualizes the distribution and renders the distribution statistics for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mu.get_features(execution_statistics)\n",
    "feature_baselines = mu.get_features(baseline_statistics)\n",
    "mu.show_distributions(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Stats vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.show_distributions(features, feature_baselines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
